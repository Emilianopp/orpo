# **ORPO**

This is the official repository for <a class="link" href="https://arxiv.org/abs/2403.07691">**Reference-free Monolithic Preference Optimization with Odds Ratio**</a>. 

**`Model Checkpoints`**

Our models trained with ORPO can be found in:

- [X] **Mistral-ORPO-‚ç∫**: <a class="link" href="https://huggingface.co/kaist-ai/mistral-orpo-alpha">kaist-ai/mistral-orpo-alpha</a>
- [X] **Mistral-ORPO-Œ≤**: <a class="link" href="https://huggingface.co/kaist-ai/mistral-orpo-beta">kaist-ai/mistral-orpo-beta</a>

**`AlpacaEval`**

<figure>
  <img class="png" src="/assets/img/ORPO_main.drawio.png" alt="Description of the image">
  <figcaption><b>Figure 1.</b> AlpacaEval\(\text{}_{2.0}\) score of <b>\(\texttt{ORPO}\)-Llama-2 (7B)</b>, ü§ó <b>Mistral-\(\texttt{ORPO}\)-\(\alpha\) (7B)</b>, and ü§ó <b>Mistral-\(\texttt{ORPO}\)-\(\beta\) (7B)</b> compared to RLHF and DPO models. They surpass RLHF and DPO-tuned models, respectively.</figcaption>
</figcaption>
</figure>

**`MT-Bench`**

<figure>
  <img class="png" src="/assets/img/mtbench_hf.png" alt="Description of the image">
  <figcaption><b>Figure 2.</b> MT-Bench result by category.</figcaption>
</figure>

